[{"content":"Self-Improving Language Models: A New Frontier in Program Synthesis Program synthesis is like teaching a computer to write its own code. Instead of giving it step-by-step instructions, you provide examples of what you want, and it figures out how to make it happen. It‚Äôs a powerful concept with huge potential, but it‚Äôs also incredibly challenging‚Äîespecially for complex tasks like those in the Abstraction and Reasoning Corpus (ARC) benchmark. ARC puzzles test an AI‚Äôs ability to reason by asking it to transform colored grids based on just a few examples, and even the best language models struggle to crack them in one shot.\nThat‚Äôs where SOAR comes in‚Äîa groundbreaking framework called Self-improving Operators for Automated program Refinements. Introduced in the paper \u0026ldquo;Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI\u0026rdquo;, SOAR helps language models get better at generating and refining programs by learning from their own attempts. This blog post dives into what SOAR is, how it works, and why it‚Äôs a game-changer for program synthesis.\nWhat‚Äôs the Big Deal with Program Synthesis? Imagine you want to automate a task but don‚Äôt know how to code it yourself. With program synthesis, you can show the computer a few examples‚Äîlike ‚Äúinput this, output that‚Äù‚Äîand it will generate a program to do the job. In ARC, these examples are visual: input grids of colored cells and their corresponding output grids. The AI has to deduce the transformation rule and write a program to apply it.\nThis is no small feat. ARC tasks often involve abstract reasoning‚Äîthink pattern recognition, spatial relationships, or even basic physics‚Äîmaking them easy for humans but tough for machines. Traditional methods and even cutting-edge language models hit a wall here, solving only a tiny fraction of these puzzles without extra help.\nWhy Current Models Fall Short Even top-tier models like GPT-4o (4.75% success), Gemini-1.5-Pro (2.75%), or Claude-3.5-Sonnet (11.25%) falter on ARC tasks when asked to solve them in a single attempt. Smaller open-source models, like Qwen-2.5-Coder-7B, fare even worse at 1%. Why? These tasks demand reasoning beyond what a one-shot guess can handle. But give these models a chance to explore multiple solutions, and things start to improve‚Äîhinting at the potential for a smarter approach.\nSOAR: A Self-Improving Framework SOAR flips the script by turning language models into self-learners. It‚Äôs built on a two-phase loop that keeps getting better with each cycle:\nSearch Phase: The model generates candidate programs and refines them using feedback from running those programs against the training examples. Learning Phase: It takes the data from those search attempts‚Äîboth successes and failures‚Äîand fine-tunes itself to improve for the next round. This creates a virtuous cycle: a sharper model leads to smarter searches, which produce richer data for further improvement. Let‚Äôs break it down.\nThe Search Phase: Exploring the Solution Space In the search phase, SOAR starts by having the language model generate a pool of possible programs‚Äîsay, 3,000 candidates. These are written in Python, not some restrictive custom language, giving the model freedom to experiment. Then, it refines these programs iteratively, using execution feedback to tweak them. Think of it like navigating a maze: the model tries different paths, learns from dead ends, and hones in on the exit.\nTo balance exploring new ideas and refining promising ones, SOAR uses a technique called REX (a bandit algorithm) and caps it off with majority voting to pick the best solution from the final pool of 6,000 candidates (half from generation, half from refinement).\nThe Learning Phase: Turning Mistakes into Lessons Here‚Äôs where SOAR shines. Instead of discarding failed attempts, it learns from them. For generation, it uses hindsight relabeling: if a program didn‚Äôt solve the original task, it‚Äôs still a valid solution for the output it did produce. This trick turns every attempt into a training example, creating a massive dataset‚Äîup to 2.4 million problem-solution pairs from ARC‚Äôs 400 training tasks.\nFor refinement, SOAR collects examples where an incorrect program was successfully fixed and uses those to teach the model how to polish its work. After fine-tuning, the model returns to the search phase smarter than before.\nIterative Improvement: A Virtuous Cycle SOAR doesn‚Äôt stop at one pass. Each iteration builds on the last, compounding gains. After four rounds, performance jumps by 13-16% across different model sizes. And it‚Äôs not just for training‚Äîit adapts to new tasks at test time, improving even without seeing the answers, thanks to a modified self-improvement loop.\nResults That Speak for Themselves The numbers tell the story. On ARC‚Äôs test set, base Qwen-2.5-Coder models (7B, 14B, 32B) solve 8-21% of tasks with search alone. After four iterations of SOAR, this leaps to 24% (7B), 31% (14B), and 34% (32B). Combine outputs from all three sizes, and SOAR hits 41.25%‚Äîa new state-of-the-art for program synthesis with open-source models on ARC.\nCompare that to Claude-3.5-Sonnet‚Äôs 11.25% in one shot, and it‚Äôs clear: SOAR lets smaller models punch above their weight through self-improvement.\nModel 1-Shot (%) Search-6k (%) SOAR After 4 Iterations (%) Claude-3.5-Sonnet 11.25 - - Qwen-2.5-Coder-7B 1.00 8.25 24.00 Qwen-2.5-Coder-14B 1.50 18.00 31.00 Qwen-2.5-Coder-32B 2.25 21.13 34.00 Combined (SOAR) - - 41.25 Why This Matters SOAR proves AI can bootstrap its own growth without mountains of human-labeled data. By learning from its own search experiences, it turns failures into stepping stones, paving the way for systems that adapt and improve on the fly. Imagine AI assistants mastering new skills as they go, or solvers tackling novel problems by building on past efforts‚ÄîSOAR makes that future feel closer.\nWhat‚Äôs Next? The paper hints at exciting possibilities: scaling SOAR to bigger models or more iterations, testing it on other benchmarks, or blending it with techniques like reinforcement learning. Could it crack even tougher reasoning tasks? Time will tell.\nConclusion SOAR is a leap forward for program synthesis. By weaving language models into a self-improving loop, it transforms static tools into dynamic learners, achieving impressive results on ARC and setting a new benchmark for open-source AI. More than that, it‚Äôs a blueprint for building systems that grow smarter over time, pushing us closer to AI that truly reasons and adapts like we do.\n","permalink":"http://localhost:1313/posts/soar/","summary":"\u003ch1 id=\"self-improving-language-models-a-new-frontier-in-program-synthesis\"\u003eSelf-Improving Language Models: A New Frontier in Program Synthesis\u003c/h1\u003e\n\u003cp\u003eProgram synthesis is like teaching a computer to write its own code. Instead of giving it step-by-step instructions, you provide examples of what you want, and it figures out how to make it happen. It‚Äôs a powerful concept with huge potential, but it‚Äôs also incredibly challenging‚Äîespecially for complex tasks like those in the Abstraction and Reasoning Corpus (ARC) benchmark. ARC puzzles test an AI‚Äôs ability to reason by asking it to transform colored grids based on just a few examples, and even the best language models struggle to crack them in one shot.\u003c/p\u003e","title":"Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI"},{"content":" Generating a Diversity of Challenging Programming Puzzles with Autotelic Generative Models (ACES) Introduction Human intelligence is marked not just by the ability to solve problems, but by the creative act of inventing them. Automating the generation of novel, diverse, and challenging problems has wide-ranging applications-from personalized education to robust benchmarking of AI systems. The ACES (Autotelic CodE Search) framework, accepted as a Spotlight Poster üí´ at NeurIPS 2024, introduces a principled method for generating Python programming puzzles that are both difficult and semantically varied, pushing the boundaries of what current generative models can achieve alone.\nWhat is ACES? ACES is an autotelic generative algorithm designed to automate the creation of Python programming puzzles. Inspired by intrinsic motivation in human learning, ACES seeks to generate puzzles that are:\nChallenging: Difficult for state-of-the-art language models to solve. Diverse: Spanning a wide range of programming skills and concepts. The ACES pipeline leverages large language models (LLMs) both as puzzle generators and solvers, iteratively refining its archive of puzzles to maximize both difficulty and diversity.\nHow Does ACES Work? 1. How to define a meaningful diversity? Measuring diversity in programming puzzles is challenging. Simple metrics like the number of lines of code do not capture meaningful conceptual differences and can be misleading, as code length often reflects style rather than substance. Using learned code embeddings can provide quantitative variety, but these representations are opaque and not easily interpretable by humans.\nTo address this, we defined diversity in a semantic, interpretable space: each puzzle is labeled with a combination of up to five out of 20 programming skill descriptors (such as recursion, dynamic programming, or array indexing), which are automatically assigned by a large language model. This approach ensures that diversity is both understandable and generalizable, as the descriptors can be adapted or extended by the LLM to cover new domains or topics. Furthermore, given this diversity space we can do goal-targeted generation of new problem given a set of semantic descriptor, this allow us to more efficiently explore this diversity space and so to explicitly maximize the diversity, whereas it is not possible given a diversity space based on embedding.\n2. Semantic Skill Space Each puzzle is represented by a set of semantic skill descriptors (e.g., recursion, string manipulation, dynamic programming). These descriptors are assigned by an LLM labeler from a curated list of 20 programming skills, ensuring that diversity is measured in a way that aligns with human intuition.\n3. Difficulty Assessment Puzzle difficulty is empirically measured using the success rate of a strong LLM solver (e.g., Llama-3-405B, Mistral-Large, Qwen-coder, \u0026hellip;), pass@1 estimated over $n$ attempts ($n=50$). A puzzle is considered more difficult if the solver rarely succeeds within n attempts and vice-versa.\n4. Iterative Quality-Diversity Search ACES builds on the Map-Elites evolutionary algorithm, maintaining an archive of puzzles grouped by their skill combinations (\u0026ldquo;niches\u0026rdquo;). The process is as follows:\nSample a Target Niche: Select a combination of skills for which to generate a new puzzle. Select Examples: Retrieve challenging puzzles from neighboring niches as few-shot examples. LLM Generation: Prompt the LLM to generate a new, harder puzzle targeting the sampled skills. Evaluation: Attempt to solve the puzzle with an LLM. Only puzzles that are solvable (but difficult) are retained. Skill Labeling: Use an LLM to assign skill descriptors to the new puzzle. Archive Update: Add the new puzzle to the appropriate niche in the archive. This loop continues, progressively filling the archive with a greater diversity of more challenging puzzles.\nBenchmark generator The ACES framework is not only a generator of diverse and challenging programming puzzles, but it is also a benchmark generator for evaluating the capabilities of code-generating language models.\nACES-generated benchmarks are built by iteratively creating archive of Python programming puzzles for a given number of iteration, this let the user choose the number of problem in the benchmark and the targeted difficulty can also modified in many ways, for example:\nincreasing the number of attempts to solve each problem during the generation will increase the difficulty using a better model You can also change the diversity space by adding or removing semantic descriptor, note that the total number of unique niches is $N_{\\text{unique niche}}=\\sum_{k=1}^{s} \\binom{n}{k}$ where $n$ is the number of semantic descriptor and $s$ is the maximum number of combination of semantic descriptor allowed for each niche. For our experiment we use $n=20$ semantic descriptor with $s=5$ semantic descriptor for each niche which make up to a possible of 21699 unique niche.\nWhy is ACES Important? 1. Outperforming Baselines ACES generates puzzles that are significantly more diverse and difficult than those produced by standard generative approaches or even existing human-curated benchmarks like HumanEval and BigCodeBench. For example, models that perform well on HumanEval experience a substantial drop in accuracy on ACES-generated puzzles, indicating that these new puzzles are genuinely more challenging.\n2. Transferability and Benchmarking The difficulty of ACES puzzles transfers across different LLMs, making them robust benchmarks for evaluating AI coding abilities. The generated benchmarks correlate more strongly with uncontaminated datasets like LiveCodeBench than with potentially saturated ones like HumanEval.\n3. Improving Model Training Finetuning LLMs on ACES-generated data leads to better performance on challenging test sets compared to training on data from other synthetic generators. As test set difficulty increases, the advantage of ACES-trained models becomes even more pronounced.\nKey Results Diversity: ACES fills up to 700 unique skill niches, far surpassing other methods. Difficulty: Generated puzzles are, on average, three times more challenging than those in existing benchmarks. Scalability: Using larger LLMs as generators and solvers further increases both diversity and difficulty. Quality-Diversity Score: ACES achieves higher QD-scores, reflecting both the breadth and challenge of its puzzle archive. Example: ACES-Generated Puzzle # This puzzle requires skills in array indexing, mathematical operations, and game theory. from typing import List def f(moves: List[List[int]], initial_state=[3, 3, 2, 2, 3, 8]) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Check if the sequence of moves solves a variant of the Nim game. Each move is a tuple (i, n) indicating removing n objects from heap i. The goal is to reach all heaps empty, with the bot playing optimally between moves. \u0026#34;\u0026#34;\u0026#34; def bot_move(): vals = sorted(state, reverse=True) i_largest = state.index(vals[0]) state[i_largest] -= max(vals[0] - vals[1], 1) state = initial_state[:] for (i, n) in moves: assert 0 \u0026lt; n \u0026lt;= state[i], \u0026#39;Illegal move\u0026#39; state[i] -= n if set(state) == {0}: return True assert any(state), \u0026#39;You lost!\u0026#39; bot_move() return set(state) == {0} def g(initial_state=[3, 3, 2, 2, 3, 8]): # Generate a sequence of optimal moves to win the game state = initial_state[:] moves = [] def losing(h): xor = 0 for i in h: xor ^= i return xor == 0 def optimal_move(): assert not losing(state) for i in range(len(state)): for n in range(1, state[i] + 1): state[i] -= n if losing(state): moves.append((i, n)) return state[i] += n assert False, \u0026#34;Shouldn\u0026#39;t reach here\u0026#34; while any(state): optimal_move() if max(state) == 0: return moves vals = sorted(state, reverse=True) i_largest = state.index(vals[0]) state[i_largest] -= max(vals[0] - vals[1], 1) return moves assert f(g()) Limitations and Future Directions Labeling Accuracy: The skill labeling process, while generally reliable, is not perfect and can sometimes misclassify the required skills.\nComputational Cost: Evaluating puzzle difficulty is expensive, requiring multiple LLM calls per puzzle.\nPotential for Hacking: There is a theoretical risk that the generator could exploit the difficulty metric, though this was not observed in experiments.\nFuture work could focus on improving the robustness of skill labeling, reducing computational costs, and extending the approach to other domains beyond programming puzzles.\nConclusion ACES represents a significant advance in the automatic generation of programming challenges. By explicitly optimizing for both diversity and difficulty in a semantic skill space, ACES produces a new generation of benchmarks that better reflect the open-ended nature of human creativity and present a more rigorous test for AI problem-solving capabilities.\nReference:\n[1] NeurIPS 2024: \u0026quot;Generating a Diversity of Challenging Programming Puzzles with Autotelic Generative Models (ACES)\u0026quot;\n","permalink":"http://localhost:1313/posts/aces/","summary":"\u003c!---\nTODO:\n- need to recheck all stuff\n- add colab demo\n- add code link\n--\u003e\n\u003ch1 id=\"generating-a-diversity-of-challenging-programming-puzzles-with-autotelic-generative-models-aces\"\u003eGenerating a Diversity of Challenging Programming Puzzles with Autotelic Generative Models (ACES)\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eHuman intelligence is marked not just by the ability to solve problems, but by the creative act of inventing them. Automating the generation of novel, diverse, and challenging problems has wide-ranging applications-from personalized education to robust benchmarking of AI systems. The \u003ca href=\"https://neurips.cc/virtual/2024/poster/95626\"\u003e\u003ccode\u003eACES\u003c/code\u003e\u003c/a\u003e (Autotelic CodE Search) framework, accepted as a \u003cstrong\u003eSpotlight Poster üí´ at NeurIPS 2024\u003c/strong\u003e, introduces a principled method for generating Python programming puzzles that are both difficult and semantically varied, pushing the boundaries of what current generative models can achieve alone.\u003c/p\u003e","title":"Generating a Diversity of Challenging Programming Puzzles with Autotelic Generative Models"}]